{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CeylonPulse: Data Collection & Signal Detection\n",
        "\n",
        "**Real-Time Situational Awareness System for Sri Lanka**\n",
        "\n",
        "This notebook implements **Step 2** of the workflow:\n",
        "- Data Collection from multiple sources\n",
        "- Signal Detection using 40 PESTLE-based signals\n",
        "- Integration with TensorFlow models (for future steps)\n",
        "\n",
        "## Three Data Collection Methods:\n",
        "1. **Scraping** - RSS feeds, web scraping\n",
        "2. **API Responses** - Twitter, Google Trends\n",
        "3. **LLM Extraction** - Structure data + generate signals\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup & Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q requests beautifulsoup4 feedparser lxml\n",
        "!pip install -q pytrends python-dateutil\n",
        "!pip install -q openai anthropic\n",
        "!pip install -q pandas numpy\n",
        "\n",
        "# For TensorFlow (for future ML models)\n",
        "!pip install -q tensorflow\n",
        "\n",
        "print(\"âœ… All packages installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive (optional - to save data)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set working directory\n",
        "import os\n",
        "os.chdir('/content')\n",
        "\n",
        "print(\"âœ… Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries & Load 40 Signals\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load 40 PESTLE signals from SSD\n",
        "SIGNALS = [\n",
        "    \"Government Policy Announcements\", \"Cabinet/Parliament Decisions\",\n",
        "    \"Government Sector Strike Warnings\", \"Police/Security Alerts\",\n",
        "    \"Election-related Discussions\", \"Foreign Policy / International Agreements\",\n",
        "    \"Tax Revision Rumors\", \"Public Protests & Demonstrations\",\n",
        "    \"Inflation Mentions\", \"Fuel Shortage Mentions\", \"Dollar Rate Discussions\",\n",
        "    \"Tourism Search Trend (Google Trends)\", \"Food Price Spikes\",\n",
        "    \"Stock Market Volatility\", \"Foreign Investment News\",\n",
        "    \"Currency Black Market Mentions\", \"Crime & Safety Alerts\",\n",
        "    \"Public Sentiment (Social Media)\", \"Migration / Visa Interest\",\n",
        "    \"Public Health Discussions\", \"Viral Social Trends\",\n",
        "    \"Cultural Event Mentions\", \"Power Outages (CEB)\",\n",
        "    \"Telecom Outages\", \"Cyberattack Mentions\",\n",
        "    \"E-commerce Growth Indicators\", \"Digital Payments Failure Reports\",\n",
        "    \"New Regulations Affecting Businesses\", \"Court Rulings Impacting Industries\",\n",
        "    \"Import/Export Restriction Changes\", \"Customs/Port Delays\",\n",
        "    \"Rainfall Alerts\", \"Flood Warnings\", \"Heat Wave Alerts\",\n",
        "    \"Landslide Warnings\", \"Cyclone Updates\", \"Air Quality Index Changes\",\n",
        "    \"Drought Warnings\", \"Water Supply Cuts (NWSDB)\",\n",
        "    \"Coastal Erosion / Tsunami Alerts\"\n",
        "]\n",
        "\n",
        "print(f\"âœ… Loaded {len(SIGNALS)} PESTLE signals\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RSS Feed Scraping\n",
        "import feedparser\n",
        "import requests\n",
        "\n",
        "def scrape_rss_feed(url):\n",
        "    \"\"\"Scrape RSS feed and return articles\"\"\"\n",
        "    try:\n",
        "        feed = feedparser.parse(url)\n",
        "        articles = []\n",
        "        \n",
        "        for entry in feed.entries:\n",
        "            article = {\n",
        "                'title': entry.get('title', ''),\n",
        "                'link': entry.get('link', ''),\n",
        "                'description': entry.get('description', ''),\n",
        "                'published': entry.get('published', ''),\n",
        "                'source': feed.feed.get('title', 'Unknown'),\n",
        "                'scraped_at': datetime.utcnow().isoformat()\n",
        "            }\n",
        "            articles.append(article)\n",
        "        \n",
        "        return articles\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping RSS feed {url}: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Data source URLs\n",
        "ADA_DERANA_RSS = 'https://www.adaderana.lk/rss.php'\n",
        "ECONOMYNEXT_RSS = 'https://economynext.com/rss'\n",
        "\n",
        "# Scrape RSS feeds\n",
        "ada_articles = scrape_rss_feed(ADA_DERANA_RSS)\n",
        "econ_articles = scrape_rss_feed(ECONOMYNEXT_RSS)\n",
        "\n",
        "all_scraped_articles = ada_articles + econ_articles\n",
        "print(f\"âœ… Scraped {len(ada_articles)} from Ada Derana, {len(econ_articles)} from EconomyNext\")\n",
        "print(f\"ðŸ“Š Total articles: {len(all_scraped_articles)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Trends API\n",
        "from pytrends.request import TrendReq\n",
        "\n",
        "def get_google_trends(geo='LK'):\n",
        "    \"\"\"Get Google Trends data for Sri Lanka\"\"\"\n",
        "    try:\n",
        "        pytrends = TrendReq(hl='en-US', tz=360)\n",
        "        trending = pytrends.trending_searches(pn=geo.lower())\n",
        "        \n",
        "        trends = []\n",
        "        for idx, trend in enumerate(trending[0].head(20).values):\n",
        "            trend_data = {\n",
        "                'rank': idx + 1,\n",
        "                'keyword': trend[0] if isinstance(trend, list) else str(trend),\n",
        "                'geo': geo,\n",
        "                'source': 'Google Trends',\n",
        "                'scraped_at': datetime.utcnow().isoformat()\n",
        "            }\n",
        "            trends.append(trend_data)\n",
        "        \n",
        "        return trends\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting Google Trends: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Get trending searches\n",
        "trends = get_google_trends('LK')\n",
        "print(f\"âœ… Retrieved {len(trends)} trending searches\")\n",
        "\n",
        "# Display top trends\n",
        "if trends:\n",
        "    df_trends = pd.DataFrame(trends)\n",
        "    print(\"\\nðŸ“ˆ Top 10 Trending Searches in Sri Lanka:\")\n",
        "    print(df_trends[['rank', 'keyword']].head(10).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Signal keywords mapping (from SSD - Signal Specification Document)\n",
        "SIGNAL_KEYWORDS = {\n",
        "    \"Government Policy Announcements\": [\"policy\", \"tax\", \"cabinet approves\", \"budget\", \"government policy\"],\n",
        "    \"Fuel Shortage Mentions\": [\"fuel shortage\", \"petrol shortage\", \"diesel shortage\", \"fuel crisis\", \"fuel queues\"],\n",
        "    \"Inflation Mentions\": [\"inflation\", \"price increase\", \"cost of living\", \"inflation rate\", \"cpi\"],\n",
        "    \"Dollar Rate Discussions\": [\"dollar rate\", \"usd rate\", \"exchange rate\", \"rupee dollar\", \"currency rate\"],\n",
        "    \"Power Outages (CEB)\": [\"power outage\", \"power cut\", \"load shedding\", \"ceb\", \"electricity cut\"],\n",
        "    \"Flood Warnings\": [\"flood\", \"flooding\", \"flood warning\", \"flood alert\", \"flash flood\"],\n",
        "    \"Public Protests & Demonstrations\": [\"protest\", \"demonstration\", \"rally\", \"march\", \"protesters\"],\n",
        "    \"Rainfall Alerts\": [\"rainfall\", \"heavy rain\", \"rain alert\", \"rainfall warning\", \"monsoon\"],\n",
        "    \"Crime & Safety Alerts\": [\"crime\", \"robbery\", \"theft\", \"murder\", \"safety alert\"],\n",
        "    \"Tourism Search Trend (Google Trends)\": [\"tourism\", \"tourist\", \"visitor\", \"travel sri lanka\", \"hotel booking\"],\n",
        "    # Add more as needed\n",
        "}\n",
        "\n",
        "def detect_signals(text, title=\"\"):\n",
        "    \"\"\"Detect signals from text using keyword matching (SSD-based)\"\"\"\n",
        "    full_text = f\"{title} {text}\".lower()\n",
        "    detected = []\n",
        "    \n",
        "    for signal_name, keywords in SIGNAL_KEYWORDS.items():\n",
        "        matches = []\n",
        "        for keyword in keywords:\n",
        "            pattern = r'\\b' + re.escape(keyword.lower()) + r'\\b'\n",
        "            if re.search(pattern, full_text):\n",
        "                matches.append(keyword)\n",
        "        \n",
        "        if matches:\n",
        "            confidence = min(0.5 + (len(matches) * 0.15), 1.0)\n",
        "            detected.append({\n",
        "                'signal_name': signal_name,\n",
        "                'confidence': round(confidence, 2),\n",
        "                'matched_keywords': matches[:5]\n",
        "            })\n",
        "    \n",
        "    return detected\n",
        "\n",
        "# Detect signals in articles\n",
        "for article in all_scraped_articles[:10]:  # Test on first 10\n",
        "    signals = detect_signals(article.get('description', ''), article.get('title', ''))\n",
        "    article['detected_signals'] = signals\n",
        "\n",
        "print(\"âœ… Signal detection completed!\")\n",
        "print(f\"ðŸ“Š Articles with signals: {sum(1 for a in all_scraped_articles[:10] if a.get('detected_signals'))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: LLM Extraction (if API key available)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mistral 7B Instruct LLM (FREE - via Hugging Face)\n",
        "USE_LLM = True  # Set to True to use Mistral 7B (free!)\n",
        "HUGGINGFACE_API_TOKEN = os.getenv('HUGGINGFACE_API_TOKEN', '')  # Optional but recommended\n",
        "\n",
        "if USE_LLM:\n",
        "    try:\n",
        "        import requests\n",
        "        \n",
        "        MISTRAL_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "        API_URL = f\"https://api-inference.huggingface.co/models/{MISTRAL_MODEL}\"\n",
        "        \n",
        "        def extract_signals_mistral(text, title=\"\"):\n",
        "            \"\"\"Extract signals using Mistral 7B Instruct (FREE!)\"\"\"\n",
        "            prompt = f\"\"\"Analyze this news article and extract relevant signals from the 40 PESTLE signals.\n",
        "\n",
        "Title: {title}\n",
        "Content: {text[:1000]}\n",
        "\n",
        "Available signals: {', '.join(SIGNALS[:15])}...\n",
        "\n",
        "Return a JSON object with a \"signals\" array. Each signal should have:\n",
        "- signal_name (must match one from the list)\n",
        "- confidence (0-1)\n",
        "- pestle_category\n",
        "- swot_category\n",
        "- severity_estimate (0-1)\n",
        "\n",
        "Format: {{\"signals\": [{{\"signal_name\": \"...\", \"confidence\": 0.8, ...}}]}}\"\"\"\n",
        "            \n",
        "            # Format for Mistral Instruct\n",
        "            formatted_prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
        "            \n",
        "            headers = {}\n",
        "            if HUGGINGFACE_API_TOKEN:\n",
        "                headers[\"Authorization\"] = f\"Bearer {HUGGINGFACE_API_TOKEN}\"\n",
        "            \n",
        "            payload = {\n",
        "                \"inputs\": formatted_prompt,\n",
        "                \"parameters\": {\n",
        "                    \"max_new_tokens\": 1000,\n",
        "                    \"temperature\": 0.3,\n",
        "                    \"return_full_text\": False\n",
        "                }\n",
        "            }\n",
        "            \n",
        "            try:\n",
        "                response = requests.post(API_URL, headers=headers, json=payload, timeout=60)\n",
        "                \n",
        "                if response.status_code == 200:\n",
        "                    result = response.json()\n",
        "                    if isinstance(result, list) and len(result) > 0:\n",
        "                        content = result[0].get('generated_text', '')\n",
        "                    else:\n",
        "                        content = str(result)\n",
        "                    \n",
        "                    # Extract JSON from response\n",
        "                    import re\n",
        "                    json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
        "                    if json_match:\n",
        "                        parsed = json.loads(json_match.group())\n",
        "                        return parsed.get('signals', [])\n",
        "                    return []\n",
        "                elif response.status_code == 503:\n",
        "                    print(\"âš ï¸ Model is loading, please wait a moment and try again\")\n",
        "                    return []\n",
        "                else:\n",
        "                    print(f\"âš ï¸ API error: {response.status_code}\")\n",
        "                    return []\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ LLM extraction error: {str(e)}\")\n",
        "                return []\n",
        "        \n",
        "        # Test on one article\n",
        "        if all_scraped_articles:\n",
        "            test_article = all_scraped_articles[0]\n",
        "            print(f\"Testing Mistral 7B on: {test_article.get('title', '')[:50]}...\")\n",
        "            llm_signals = extract_signals_mistral(\n",
        "                test_article.get('description', ''),\n",
        "                test_article.get('title', '')\n",
        "            )\n",
        "            print(f\"âœ… Mistral 7B extracted {len(llm_signals)} signals\")\n",
        "            if llm_signals:\n",
        "                print(f\"   Example: {llm_signals[0].get('signal_name', 'N/A')}\")\n",
        "    except ImportError:\n",
        "        print(\"âš ï¸ Requests library not available\")\n",
        "else:\n",
        "    print(\"âš ï¸ LLM extraction disabled\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Data & Prepare for TensorFlow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine all data\n",
        "all_data = all_scraped_articles + trends\n",
        "\n",
        "# Save to JSON\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "output_file = f'/content/collected_data_{timestamp}.json'\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"âœ… Saved {len(all_data)} items to {output_file}\")\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(all_data)\n",
        "print(f\"\\nðŸ“Š Data Summary:\")\n",
        "print(f\"Total items: {len(df)}\")\n",
        "if 'source' in df.columns:\n",
        "    print(f\"\\nSources:\\n{df['source'].value_counts()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import TensorFlow for future ML models\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print(f\"âœ… TensorFlow {tf.__version__} imported\")\n",
        "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
        "\n",
        "# Text preprocessing for TensorFlow\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Basic text preprocessing\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return text.lower().strip()\n",
        "\n",
        "# Preprocess text\n",
        "if 'description' in df.columns:\n",
        "    df['processed_text'] = df['description'].apply(preprocess_text)\n",
        "elif 'text' in df.columns:\n",
        "    df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "print(\"âœ… Text preprocessing completed - ready for TensorFlow models!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "âœ… **Data Collection Complete!**\n",
        "\n",
        "- **Method 1 (Scraping)**: RSS feeds from Ada Derana & EconomyNext\n",
        "- **Method 2 (API)**: Google Trends for Sri Lanka\n",
        "- **Method 3 (Signal Detection)**: Keyword-based detection from SSD\n",
        "\n",
        "**Next Steps** (from Workflow.md):\n",
        "- Step 3: NLP Preprocessing (SBERT embeddings, clustering)\n",
        "- Step 4: Deep Learning Models (BERT, LSTM)\n",
        "- Step 5: Model Training\n",
        "- Step 6: Model Evaluation\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
