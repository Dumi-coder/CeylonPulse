{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üá±üá∞ CeylonPulse: Complete Data Collection System\n",
        "\n",
        "**Real-Time Situational Awareness System for Sri Lanka**\n",
        "\n",
        "This notebook contains **ALL** functionality from the Python modules - everything runs in Colab!\n",
        "\n",
        "## Features:\n",
        "- ‚úÖ RSS Feed Scraping\n",
        "- ‚úÖ Web Scraping  \n",
        "- ‚úÖ Google Trends API\n",
        "- ‚úÖ Twitter API (optional)\n",
        "- ‚úÖ Signal Detection (40 PESTLE signals)\n",
        "- ‚úÖ Mistral 7B LLM Extraction\n",
        "- ‚úÖ Data Storage (JSON)\n",
        "- ‚úÖ TensorFlow Ready\n",
        "\n",
        "**No need for local Python files - everything is here!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Step 1: Install All Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install all required packages\n",
        "%pip install -q requests beautifulsoup4 feedparser lxml\n",
        "%pip install -q pytrends python-dateutil\n",
        "%pip install -q pandas numpy\n",
        "%pip install -q tensorflow\n",
        "\n",
        "print(\"‚úÖ All packages installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Step 2: Configuration & Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import feedparser\n",
        "\n",
        "# Hugging Face Token (for Mistral 7B)\n",
        "HUGGINGFACE_API_TOKEN = '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'\n",
        "\n",
        "# Configuration\n",
        "USE_LLM = True  # Set to True to use Mistral 7B\n",
        "USE_GOOGLE_TRENDS = True\n",
        "USE_TWITTER = False  # Set to True if you have Twitter token\n",
        "\n",
        "print(\"‚úÖ Libraries imported!\")\n",
        "print(f\"‚úÖ Hugging Face token configured\")\n",
        "print(f\"‚úÖ LLM Extraction: {'Enabled' if USE_LLM else 'Disabled'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Step 3: Load 40 PESTLE Signals & Data Sources\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# All 40 PESTLE Signals (from SSD)\n",
        "SIGNALS = [\n",
        "    \"Government Policy Announcements\", \"Cabinet/Parliament Decisions\",\n",
        "    \"Government Sector Strike Warnings\", \"Police/Security Alerts\",\n",
        "    \"Election-related Discussions\", \"Foreign Policy / International Agreements\",\n",
        "    \"Tax Revision Rumors\", \"Public Protests & Demonstrations\",\n",
        "    \"Inflation Mentions\", \"Fuel Shortage Mentions\", \"Dollar Rate Discussions\",\n",
        "    \"Tourism Search Trend (Google Trends)\", \"Food Price Spikes\",\n",
        "    \"Stock Market Volatility\", \"Foreign Investment News\",\n",
        "    \"Currency Black Market Mentions\", \"Crime & Safety Alerts\",\n",
        "    \"Public Sentiment (Social Media)\", \"Migration / Visa Interest\",\n",
        "    \"Public Health Discussions\", \"Viral Social Trends\",\n",
        "    \"Cultural Event Mentions\", \"Power Outages (CEB)\",\n",
        "    \"Telecom Outages\", \"Cyberattack Mentions\",\n",
        "    \"E-commerce Growth Indicators\", \"Digital Payments Failure Reports\",\n",
        "    \"New Regulations Affecting Businesses\", \"Court Rulings Impacting Industries\",\n",
        "    \"Import/Export Restriction Changes\", \"Customs/Port Delays\",\n",
        "    \"Rainfall Alerts\", \"Flood Warnings\", \"Heat Wave Alerts\",\n",
        "    \"Landslide Warnings\", \"Cyclone Updates\", \"Air Quality Index Changes\",\n",
        "    \"Drought Warnings\", \"Water Supply Cuts (NWSDB)\",\n",
        "    \"Coastal Erosion / Tsunami Alerts\"\n",
        "]\n",
        "\n",
        "# Data Source URLs\n",
        "DATA_SOURCES = {\n",
        "    'ada_derana': {\n",
        "        'rss_feed': 'https://www.adaderana.lk/rss.php',\n",
        "        'news_page': 'https://www.adaderana.lk/news.php',\n",
        "        'breaking_news': 'https://www.adaderana.lk/breaking-news',\n",
        "        'business': 'https://www.adaderana.lk/business-news'\n",
        "    },\n",
        "    'economynext': {\n",
        "        'rss_feed': 'https://economynext.com/rss',\n",
        "        'main_site': 'https://economynext.com/',\n",
        "        'sri_lanka_news': 'https://economynext.com/c/sri-lanka',\n",
        "        'business': 'https://economynext.com/c/business'\n",
        "    },\n",
        "    'met_department': {\n",
        "        'warnings': 'http://www.meteo.gov.lk/index.php?option=com_content&view=article&id=94&Itemid=310&lang=en',\n",
        "        'weather_forecast': 'http://www.meteo.gov.lk/index.php?option=com_content&view=article&id=96&Itemid=512&lang=en'\n",
        "    },\n",
        "    'central_bank': {\n",
        "        'main_site': 'https://www.cbsl.gov.lk/',\n",
        "        'news': 'https://www.cbsl.gov.lk/news',\n",
        "        'statistics': 'https://www.cbsl.gov.lk/statistics'\n",
        "    },\n",
        "    'ceb': {\n",
        "        'outage_notices': 'https://ceb.lk/outage-notices',\n",
        "        'load_shedding': 'https://ceb.lk/load-shedding-schedule'\n",
        "    },\n",
        "    'nwsdb': {\n",
        "        'announcements': 'https://www.waterboard.lk/announcements.html',\n",
        "        'water_interruptions': 'https://www.waterboard.lk/water_interruptions.html'\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(SIGNALS)} PESTLE signals\")\n",
        "print(f\"‚úÖ Configured {len(DATA_SOURCES)} data sources\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scrape_rss_feed(url, source_name=\"Unknown\"):\n",
        "    \"\"\"Scrape RSS feed and return articles\"\"\"\n",
        "    try:\n",
        "        feed = feedparser.parse(url)\n",
        "        articles = []\n",
        "        \n",
        "        for entry in feed.entries:\n",
        "            article = {\n",
        "                'title': entry.get('title', ''),\n",
        "                'link': entry.get('link', ''),\n",
        "                'description': entry.get('description', ''),\n",
        "                'published': entry.get('published', ''),\n",
        "                'published_parsed': entry.get('published_parsed'),\n",
        "                'source': feed.feed.get('title', source_name),\n",
        "                'source_url': url,\n",
        "                'author': entry.get('author', ''),\n",
        "                'tags': [tag.get('term', '') for tag in entry.get('tags', [])],\n",
        "                'scraped_at': datetime.utcnow().isoformat()\n",
        "            }\n",
        "            articles.append(article)\n",
        "        \n",
        "        return articles\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error scraping RSS feed {url}: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Scrape RSS feeds\n",
        "print(\"Scraping RSS feeds...\")\n",
        "all_articles = []\n",
        "\n",
        "# Ada Derana\n",
        "ada_articles = scrape_rss_feed(DATA_SOURCES['ada_derana']['rss_feed'], 'Ada Derana')\n",
        "all_articles.extend(ada_articles)\n",
        "print(f\"‚úÖ Scraped {len(ada_articles)} articles from Ada Derana\")\n",
        "\n",
        "# EconomyNext\n",
        "econ_articles = scrape_rss_feed(DATA_SOURCES['economynext']['rss_feed'], 'EconomyNext')\n",
        "all_articles.extend(econ_articles)\n",
        "print(f\"‚úÖ Scraped {len(econ_articles)} articles from EconomyNext\")\n",
        "\n",
        "print(f\"\\nüìä Total articles scraped: {len(all_articles)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if USE_GOOGLE_TRENDS:\n",
        "    try:\n",
        "        from pytrends.request import TrendReq\n",
        "        \n",
        "        def get_google_trends(geo='LK'):\n",
        "            \"\"\"Get Google Trends data for Sri Lanka\"\"\"\n",
        "            try:\n",
        "                pytrends = TrendReq(hl='en-US', tz=360)\n",
        "                trending = pytrends.trending_searches(pn=geo.lower())\n",
        "                \n",
        "                trends = []\n",
        "                for idx, trend in enumerate(trending[0].head(20).values):\n",
        "                    trend_data = {\n",
        "                        'rank': idx + 1,\n",
        "                        'keyword': trend[0] if isinstance(trend, list) else str(trend),\n",
        "                        'geo': geo,\n",
        "                        'source': 'Google Trends',\n",
        "                        'scraped_at': datetime.utcnow().isoformat()\n",
        "                    }\n",
        "                    trends.append(trend_data)\n",
        "                \n",
        "                return trends\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error getting Google Trends: {str(e)}\")\n",
        "                return []\n",
        "        \n",
        "        # Get trending searches\n",
        "        trends = get_google_trends('LK')\n",
        "        print(f\"‚úÖ Retrieved {len(trends)} trending searches from Google Trends\")\n",
        "        \n",
        "        if trends:\n",
        "            df_trends = pd.DataFrame(trends)\n",
        "            print(\"\\nüìà Top 10 Trending Searches in Sri Lanka:\")\n",
        "            print(df_trends[['rank', 'keyword']].head(10).to_string(index=False))\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Google Trends not available: {str(e)}\")\n",
        "        trends = []\n",
        "else:\n",
        "    trends = []\n",
        "    print(\"‚ö†Ô∏è Google Trends disabled\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Step 6: Signal Detection (Keyword-based from SSD)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Signal keywords mapping (from SSD - Signal Specification Document)\n",
        "SIGNAL_KEYWORDS = {\n",
        "    # Political Signals\n",
        "    \"Government Policy Announcements\": [\"policy\", \"tax\", \"cabinet approves\", \"budget\", \"government policy\"],\n",
        "    \"Cabinet/Parliament Decisions\": [\"cabinet decision\", \"parliament decision\", \"cabinet meeting\", \"parliament approves\"],\n",
        "    \"Government Sector Strike Warnings\": [\"strike\", \"trade union\", \"government sector strike\", \"union warning\"],\n",
        "    \"Police/Security Alerts\": [\"police alert\", \"security alert\", \"police warning\", \"security threat\"],\n",
        "    \"Election-related Discussions\": [\"election\", \"voting\", \"poll\", \"election campaign\"],\n",
        "    \"Foreign Policy / International Agreements\": [\"foreign policy\", \"international agreement\", \"bilateral agreement\"],\n",
        "    \"Tax Revision Rumors\": [\"tax revision\", \"tax increase\", \"tax cut\", \"tax change\"],\n",
        "    \"Public Protests & Demonstrations\": [\"protest\", \"demonstration\", \"rally\", \"march\", \"protesters\"],\n",
        "    \n",
        "    # Economic Signals\n",
        "    \"Inflation Mentions\": [\"inflation\", \"price increase\", \"cost of living\", \"inflation rate\", \"cpi\"],\n",
        "    \"Fuel Shortage Mentions\": [\"fuel shortage\", \"petrol shortage\", \"diesel shortage\", \"fuel crisis\", \"fuel queues\"],\n",
        "    \"Dollar Rate Discussions\": [\"dollar rate\", \"usd rate\", \"exchange rate\", \"rupee dollar\", \"currency rate\"],\n",
        "    \"Tourism Search Trend (Google Trends)\": [\"tourism\", \"tourist\", \"visitor\", \"travel sri lanka\", \"hotel booking\"],\n",
        "    \"Food Price Spikes\": [\"food price\", \"rice price\", \"vegetable price\", \"price spike\"],\n",
        "    \"Stock Market Volatility\": [\"stock market\", \"cse\", \"share market\", \"market volatility\"],\n",
        "    \"Foreign Investment News\": [\"foreign investment\", \"fdi\", \"foreign direct investment\"],\n",
        "    \"Currency Black Market Mentions\": [\"black market\", \"underground market\", \"illegal currency\"],\n",
        "    \n",
        "    # Social Signals\n",
        "    \"Crime & Safety Alerts\": [\"crime\", \"robbery\", \"theft\", \"murder\", \"safety alert\"],\n",
        "    \"Public Sentiment (Social Media)\": [\"public sentiment\", \"social media\", \"twitter\", \"facebook\"],\n",
        "    \"Migration / Visa Interest\": [\"migration\", \"emigration\", \"visa\", \"immigration\"],\n",
        "    \"Public Health Discussions\": [\"disease\", \"outbreak\", \"epidemic\", \"health alert\"],\n",
        "    \"Viral Social Trends\": [\"viral\", \"trending\", \"social media trend\"],\n",
        "    \"Cultural Event Mentions\": [\"cultural event\", \"festival\", \"celebration\"],\n",
        "    \n",
        "    # Technological Signals\n",
        "    \"Power Outages (CEB)\": [\"power outage\", \"power cut\", \"load shedding\", \"ceb\", \"electricity cut\"],\n",
        "    \"Telecom Outages\": [\"telecom outage\", \"internet outage\", \"network outage\"],\n",
        "    \"Cyberattack Mentions\": [\"cyberattack\", \"cyber attack\", \"hacking\", \"data breach\"],\n",
        "    \"E-commerce Growth Indicators\": [\"e-commerce\", \"online shopping\", \"digital commerce\"],\n",
        "    \"Digital Payments Failure Reports\": [\"payment failure\", \"digital payment\", \"payment system down\"],\n",
        "    \n",
        "    # Legal Signals\n",
        "    \"New Regulations Affecting Businesses\": [\"regulation\", \"new regulation\", \"business regulation\"],\n",
        "    \"Court Rulings Impacting Industries\": [\"court ruling\", \"court decision\", \"legal ruling\"],\n",
        "    \"Import/Export Restriction Changes\": [\"import restriction\", \"export restriction\", \"import ban\"],\n",
        "    \"Customs/Port Delays\": [\"customs delay\", \"port delay\", \"customs clearance\"],\n",
        "    \n",
        "    # Environmental Signals\n",
        "    \"Rainfall Alerts\": [\"rainfall\", \"heavy rain\", \"rain alert\", \"rainfall warning\", \"monsoon\"],\n",
        "    \"Flood Warnings\": [\"flood\", \"flooding\", \"flood warning\", \"flood alert\", \"flash flood\"],\n",
        "    \"Heat Wave Alerts\": [\"heat wave\", \"heatwave\", \"extreme heat\", \"high temperature\"],\n",
        "    \"Landslide Warnings\": [\"landslide\", \"landslide warning\", \"mudslide\"],\n",
        "    \"Cyclone Updates\": [\"cyclone\", \"tropical cyclone\", \"storm\", \"cyclone warning\"],\n",
        "    \"Air Quality Index Changes\": [\"air quality\", \"aqi\", \"air pollution\"],\n",
        "    \"Drought Warnings\": [\"drought\", \"drought warning\", \"water shortage\"],\n",
        "    \"Water Supply Cuts (NWSDB)\": [\"water supply cut\", \"water cut\", \"water interruption\", \"nwsdb\"],\n",
        "    \"Coastal Erosion / Tsunami Alerts\": [\"tsunami\", \"tsunami alert\", \"tsunami warning\", \"coastal erosion\"]\n",
        "}\n",
        "\n",
        "def detect_signals(text, title=\"\", source=\"\"):\n",
        "    \"\"\"Detect signals from text using keyword matching (SSD-based)\"\"\"\n",
        "    if not text and not title:\n",
        "        return []\n",
        "    \n",
        "    full_text = f\"{title} {text}\".lower()\n",
        "    detected = []\n",
        "    \n",
        "    for signal_name, keywords in SIGNAL_KEYWORDS.items():\n",
        "        matches = []\n",
        "        for keyword in keywords:\n",
        "            pattern = r'\\b' + re.escape(keyword.lower()) + r'\\b'\n",
        "            if re.search(pattern, full_text):\n",
        "                matches.append(keyword)\n",
        "        \n",
        "        # Source-specific detection\n",
        "        source_match = False\n",
        "        source_lower = source.lower()\n",
        "        signal_lower = signal_name.lower()\n",
        "        \n",
        "        if \"power outage\" in signal_lower or \"ceb\" in signal_lower:\n",
        "            if \"ceb\" in source_lower or \"electricity\" in source_lower:\n",
        "                source_match = True\n",
        "        if \"water supply\" in signal_lower or \"nwsdb\" in signal_lower:\n",
        "            if \"nwsdb\" in source_lower or \"water board\" in source_lower:\n",
        "                source_match = True\n",
        "        if any(term in signal_lower for term in [\"rainfall\", \"flood\", \"cyclone\"]):\n",
        "            if \"met\" in source_lower or \"meteorological\" in source_lower:\n",
        "                source_match = True\n",
        "        \n",
        "        if matches or source_match:\n",
        "            confidence = min(0.5 + (len(matches) * 0.15) + (0.2 if source_match else 0), 1.0)\n",
        "            detected.append({\n",
        "                'signal_name': signal_name,\n",
        "                'confidence': round(confidence, 2),\n",
        "                'matched_keywords': matches[:5],\n",
        "                'source_specific_match': source_match\n",
        "            })\n",
        "    \n",
        "    # Sort by confidence\n",
        "    detected.sort(key=lambda x: x['confidence'], reverse=True)\n",
        "    return detected\n",
        "\n",
        "# Detect signals in all articles\n",
        "print(\"Detecting signals in articles...\")\n",
        "for article in all_articles:\n",
        "    signals = detect_signals(\n",
        "        article.get('description', ''),\n",
        "        article.get('title', ''),\n",
        "        article.get('source', '')\n",
        "    )\n",
        "    article['detected_signals'] = signals\n",
        "    article['signal_count'] = len(signals)\n",
        "\n",
        "articles_with_signals = sum(1 for a in all_articles if a.get('detected_signals'))\n",
        "print(f\"‚úÖ Signal detection completed!\")\n",
        "print(f\"üìä Articles with signals: {articles_with_signals} / {len(all_articles)}\")\n",
        "\n",
        "# Show sample\n",
        "if articles_with_signals > 0:\n",
        "    sample = next((a for a in all_articles if a.get('detected_signals')), None)\n",
        "    if sample:\n",
        "        print(f\"\\nüìù Sample detection:\")\n",
        "        print(f\"   Title: {sample['title'][:60]}...\")\n",
        "        print(f\"   Signals: {[s['signal_name'] for s in sample['detected_signals'][:3]]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if USE_LLM:\n",
        "    MISTRAL_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "    API_URL = f\"https://api-inference.huggingface.co/models/{MISTRAL_MODEL}\"\n",
        "    \n",
        "    def extract_signals_mistral(text, title=\"\"):\n",
        "        \"\"\"Extract signals using Mistral 7B Instruct\"\"\"\n",
        "        prompt = f\"\"\"Analyze this news article and extract relevant signals from the 40 PESTLE signals.\n",
        "\n",
        "Title: {title}\n",
        "Content: {text[:1000]}\n",
        "\n",
        "Available signals: {', '.join(SIGNALS[:20])}...\n",
        "\n",
        "For each relevant signal found, return JSON:\n",
        "{{\"signals\": [\n",
        "  {{\n",
        "    \"signal_name\": \"Signal Name (must match exactly from list)\",\n",
        "    \"confidence\": 0.8,\n",
        "    \"pestle_category\": \"Political/Economic/Social/Technological/Legal/Environmental\",\n",
        "    \"swot_category\": \"Threat/Opportunity/Weakness/Strength\",\n",
        "    \"severity_estimate\": 0.7,\n",
        "    \"key_phrases\": [\"phrase1\", \"phrase2\"]\n",
        "  }}\n",
        "]}}\n",
        "\n",
        "Only return valid JSON, nothing else.\"\"\"\n",
        "        \n",
        "        # Format for Mistral Instruct\n",
        "        formatted_prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
        "        \n",
        "        headers = {}\n",
        "        if HUGGINGFACE_API_TOKEN:\n",
        "            headers[\"Authorization\"] = f\"Bearer {HUGGINGFACE_API_TOKEN}\"\n",
        "        \n",
        "        payload = {\n",
        "            \"inputs\": formatted_prompt,\n",
        "            \"parameters\": {\n",
        "                \"max_new_tokens\": 1000,\n",
        "                \"temperature\": 0.3,\n",
        "                \"return_full_text\": False\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            response = requests.post(API_URL, headers=headers, json=payload, timeout=90)\n",
        "            \n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                if isinstance(result, list) and len(result) > 0:\n",
        "                    content = result[0].get('generated_text', '')\n",
        "                else:\n",
        "                    content = str(result)\n",
        "                \n",
        "                # Extract JSON from response\n",
        "                json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
        "                if json_match:\n",
        "                    parsed = json.loads(json_match.group())\n",
        "                    return parsed.get('signals', [])\n",
        "                return []\n",
        "            elif response.status_code == 503:\n",
        "                print(\"‚ö†Ô∏è Model is loading, please wait 30-60 seconds and try again\")\n",
        "                return []\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è API error: {response.status_code}\")\n",
        "                return []\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è LLM extraction error: {str(e)}\")\n",
        "            return []\n",
        "    \n",
        "    # Extract signals using LLM (test on first 5 articles)\n",
        "    print(\"Extracting signals using Mistral 7B...\")\n",
        "    print(\"(First request may take 30-60 seconds - model loading)\")\n",
        "    \n",
        "    llm_extracted_count = 0\n",
        "    for i, article in enumerate(all_articles[:5]):  # Test on first 5\n",
        "        text = article.get('description', '')\n",
        "        title = article.get('title', '')\n",
        "        \n",
        "        if text or title:\n",
        "            llm_signals = extract_signals_mistral(text, title)\n",
        "            if llm_signals:\n",
        "                # Merge with keyword-detected signals\n",
        "                existing_signals = article.get('detected_signals', [])\n",
        "                existing_names = {s['signal_name'] for s in existing_signals}\n",
        "                \n",
        "                for llm_sig in llm_signals:\n",
        "                    if llm_sig.get('signal_name') not in existing_names:\n",
        "                        existing_signals.append({\n",
        "                            'signal_name': llm_sig.get('signal_name', ''),\n",
        "                            'confidence': llm_sig.get('confidence', 0.0),\n",
        "                            'detection_method': 'llm',\n",
        "                            'pestle_category': llm_sig.get('pestle_category', ''),\n",
        "                            'swot_category': llm_sig.get('swot_category', ''),\n",
        "                            'severity_estimate': llm_sig.get('severity_estimate', 0.0)\n",
        "                        })\n",
        "                \n",
        "                article['detected_signals'] = existing_signals\n",
        "                article['signal_count'] = len(existing_signals)\n",
        "                llm_extracted_count += 1\n",
        "                print(f\"  ‚úÖ Article {i+1}: Extracted {len(llm_signals)} additional signals\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ LLM extraction completed on {llm_extracted_count} articles\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è LLM extraction disabled\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine all data\n",
        "all_data = all_articles + trends\n",
        "\n",
        "# Save to JSON\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "output_file = f'/content/collected_data_{timestamp}.json'\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"‚úÖ Saved {len(all_data)} items to {output_file}\")\n",
        "\n",
        "# Also save to Drive if mounted\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    \n",
        "    drive_file = f'/content/drive/MyDrive/CeylonPulse/data/collected_data_{timestamp}.json'\n",
        "    os.makedirs(os.path.dirname(drive_file), exist_ok=True)\n",
        "    with open(drive_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"‚úÖ Also saved to Drive: {drive_file}\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è Drive not mounted (optional)\")\n",
        "\n",
        "# Create DataFrame for analysis\n",
        "df = pd.DataFrame(all_data)\n",
        "print(f\"\\nüìä Data Summary:\")\n",
        "print(f\"Total items: {len(df)}\")\n",
        "if 'source' in df.columns:\n",
        "    print(f\"\\nSources:\")\n",
        "    print(df['source'].value_counts())\n",
        "\n",
        "# Signal statistics\n",
        "if 'detected_signals' in df.columns:\n",
        "    all_signals = []\n",
        "    for item in all_data:\n",
        "        if item.get('detected_signals'):\n",
        "            all_signals.extend(item['detected_signals'])\n",
        "    \n",
        "    if all_signals:\n",
        "        signal_counts = Counter(s['signal_name'] for s in all_signals)\n",
        "        print(f\"\\nüìà Top 10 Detected Signals:\")\n",
        "        for signal, count in signal_counts.most_common(10):\n",
        "            print(f\"   {signal}: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Step 9: Prepare for TensorFlow (NLP Preprocessing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import TensorFlow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print(f\"‚úÖ TensorFlow {tf.__version__} imported\")\n",
        "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
        "\n",
        "# Text preprocessing for TensorFlow\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Basic text preprocessing\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Remove special characters (keep alphanumeric and spaces)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Lowercase and strip\n",
        "    return text.lower().strip()\n",
        "\n",
        "# Preprocess all text data\n",
        "if 'description' in df.columns:\n",
        "    df['processed_text'] = df['description'].apply(preprocess_text)\n",
        "elif 'text' in df.columns:\n",
        "    df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "print(\"‚úÖ Text preprocessing completed - ready for TensorFlow models!\")\n",
        "print(f\"\\nSample processed text:\")\n",
        "if 'processed_text' in df.columns and len(df) > 0:\n",
        "    sample = df['processed_text'].iloc[0]\n",
        "    print(f\"   {sample[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 10: Summary & Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"CeylonPulse Data Collection Summary\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"‚úÖ Total items collected: {len(all_data)}\")\n",
        "print(f\"   - Articles from RSS: {len(all_articles)}\")\n",
        "print(f\"   - Trends from Google: {len(trends)}\")\n",
        "print(f\"\\n‚úÖ Signal Detection:\")\n",
        "print(f\"   - Articles with signals: {articles_with_signals}\")\n",
        "print(f\"   - Total signal detections: {sum(len(a.get('detected_signals', [])) for a in all_articles)}\")\n",
        "print(f\"\\n‚úÖ Data Storage:\")\n",
        "print(f\"   - Saved to: {output_file}\")\n",
        "print(f\"   - File size: {os.path.getsize(output_file) / 1024:.1f} KB\")\n",
        "print(f\"\\n‚úÖ Next Steps:\")\n",
        "print(\"   - Review collected data\")\n",
        "print(\"   - Proceed to Step 3: NLP Preprocessing (SBERT embeddings)\")\n",
        "print(\"   - Proceed to Step 4: Deep Learning Models (BERT, LSTM)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Display sample data\n",
        "if len(all_data) > 0:\n",
        "    print(f\"\\nüìù Sample Article:\")\n",
        "    sample = all_data[0]\n",
        "    print(f\"   Title: {sample.get('title', 'N/A')[:70]}...\")\n",
        "    print(f\"   Source: {sample.get('source', 'N/A')}\")\n",
        "    if sample.get('detected_signals'):\n",
        "        print(f\"   Signals: {[s['signal_name'] for s in sample['detected_signals'][:3]]}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
