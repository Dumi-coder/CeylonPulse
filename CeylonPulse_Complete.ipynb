{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LANcddmnwjvq"
      },
      "source": [
        "# üá±üá∞ CeylonPulse: Complete Data Collection System\n",
        "\n",
        "**Real-Time Situational Awareness System for Sri Lanka**\n",
        "\n",
        "This notebook contains **ALL** functionality from the Python modules - everything runs in Colab!\n",
        "\n",
        "## Features:\n",
        "- ‚úÖ RSS Feed Scraping\n",
        "- ‚úÖ Web Scraping  \n",
        "- ‚úÖ Google Trends API\n",
        "- ‚úÖ Twitter API (optional)\n",
        "- ‚úÖ Signal Detection (40 PESTLE signals)\n",
        "- ‚úÖ Mistral 7B LLM Extraction\n",
        "- ‚úÖ Data Storage (JSON)\n",
        "- ‚úÖ TensorFlow Ready\n",
        "\n",
        "**No need for local Python files - everything is here!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ztF1Rqjwjvt"
      },
      "source": [
        "## üì¶ Step 1: Install All Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Qle2uKvwjvu",
        "outputId": "38292ef7-3cfd-4719-c987-9f58131c5961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/81.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "‚úÖ All packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install all required packages\n",
        "%pip install -q requests beautifulsoup4 feedparser lxml\n",
        "%pip install -q pytrends python-dateutil\n",
        "%pip install -q pandas numpy\n",
        "%pip install -q tensorflow\n",
        "\n",
        "print(\"‚úÖ All packages installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApNwxjK0wjvx"
      },
      "source": [
        "## üîß Step 2: Configuration & Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7gNlV9vwjvy",
        "outputId": "fa85aee4-ff86-4fe1-db4a-082ea7c5e4eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Libraries imported!\n",
            "‚úÖ Hugging Face token configured\n",
            "‚úÖ LLM Extraction: Enabled\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import feedparser\n",
        "\n",
        "# Hugging Face Token (for Mistral 7B)\n",
        "HUGGINGFACE_API_TOKEN = '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'\n",
        "\n",
        "# Configuration\n",
        "USE_LLM = True  # Set to True to use Mistral 7B\n",
        "USE_GOOGLE_TRENDS = True\n",
        "USE_TWITTER = False  # Set to True if you have Twitter token\n",
        "\n",
        "print(\"‚úÖ Libraries imported!\")\n",
        "print(f\"‚úÖ Hugging Face token configured\")\n",
        "print(f\"‚úÖ LLM Extraction: {'Enabled' if USE_LLM else 'Disabled'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meTghfsCwjvz"
      },
      "source": [
        "## üìã Step 3: Load 40 PESTLE Signals & Data Sources\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRsr227rwjv0",
        "outputId": "50cea427-83f7-41cc-c663-b8ccc3cc6c41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded 40 PESTLE signals\n",
            "‚úÖ Configured 6 data sources\n"
          ]
        }
      ],
      "source": [
        "# All 40 PESTLE Signals (from SSD)\n",
        "SIGNALS = [\n",
        "    \"Government Policy Announcements\", \"Cabinet/Parliament Decisions\",\n",
        "    \"Government Sector Strike Warnings\", \"Police/Security Alerts\",\n",
        "    \"Election-related Discussions\", \"Foreign Policy / International Agreements\",\n",
        "    \"Tax Revision Rumors\", \"Public Protests & Demonstrations\",\n",
        "    \"Inflation Mentions\", \"Fuel Shortage Mentions\", \"Dollar Rate Discussions\",\n",
        "    \"Tourism Search Trend (Google Trends)\", \"Food Price Spikes\",\n",
        "    \"Stock Market Volatility\", \"Foreign Investment News\",\n",
        "    \"Currency Black Market Mentions\", \"Crime & Safety Alerts\",\n",
        "    \"Public Sentiment (Social Media)\", \"Migration / Visa Interest\",\n",
        "    \"Public Health Discussions\", \"Viral Social Trends\",\n",
        "    \"Cultural Event Mentions\", \"Power Outages (CEB)\",\n",
        "    \"Telecom Outages\", \"Cyberattack Mentions\",\n",
        "    \"E-commerce Growth Indicators\", \"Digital Payments Failure Reports\",\n",
        "    \"New Regulations Affecting Businesses\", \"Court Rulings Impacting Industries\",\n",
        "    \"Import/Export Restriction Changes\", \"Customs/Port Delays\",\n",
        "    \"Rainfall Alerts\", \"Flood Warnings\", \"Heat Wave Alerts\",\n",
        "    \"Landslide Warnings\", \"Cyclone Updates\", \"Air Quality Index Changes\",\n",
        "    \"Drought Warnings\", \"Water Supply Cuts (NWSDB)\",\n",
        "    \"Coastal Erosion / Tsunami Alerts\"\n",
        "]\n",
        "\n",
        "# Data Source URLs\n",
        "DATA_SOURCES = {\n",
        "    'ada_derana': {\n",
        "        'rss_feed': 'https://www.adaderana.lk/rss.php',\n",
        "        'news_page': 'https://www.adaderana.lk/news.php',\n",
        "        'breaking_news': 'https://www.adaderana.lk/breaking-news',\n",
        "        'business': 'https://www.adaderana.lk/business-news'\n",
        "    },\n",
        "    'economynext': {\n",
        "        'rss_feed': 'https://economynext.com/rss',\n",
        "        'main_site': 'https://economynext.com/',\n",
        "        'sri_lanka_news': 'https://economynext.com/c/sri-lanka',\n",
        "        'business': 'https://economynext.com/c/business'\n",
        "    },\n",
        "    'met_department': {\n",
        "        'warnings': 'http://www.meteo.gov.lk/index.php?option=com_content&view=article&id=94&Itemid=310&lang=en',\n",
        "        'weather_forecast': 'http://www.meteo.gov.lk/index.php?option=com_content&view=article&id=96&Itemid=512&lang=en'\n",
        "    },\n",
        "    'central_bank': {\n",
        "        'main_site': 'https://www.cbsl.gov.lk/',\n",
        "        'news': 'https://www.cbsl.gov.lk/news',\n",
        "        'statistics': 'https://www.cbsl.gov.lk/statistics'\n",
        "    },\n",
        "    'ceb': {\n",
        "        'outage_notices': 'https://ceb.lk/outage-notices',\n",
        "        'load_shedding': 'https://ceb.lk/load-shedding-schedule'\n",
        "    },\n",
        "    'nwsdb': {\n",
        "        'announcements': 'https://www.waterboard.lk/announcements.html',\n",
        "        'water_interruptions': 'https://www.waterboard.lk/water_interruptions.html'\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(SIGNALS)} PESTLE signals\")\n",
        "print(f\"‚úÖ Configured {len(DATA_SOURCES)} data sources\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wei_kvEnwjv1"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5EBpoqWwjv3",
        "outputId": "beabcec1-8d42-43f0-dcb5-d5777a6cbf25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping RSS feeds...\n",
            "‚úÖ Scraped 20 articles from Ada Derana\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4003365980.py:18: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  'scraped_at': datetime.utcnow().isoformat()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Scraped 20 articles from EconomyNext\n",
            "\n",
            "üìä Total articles scraped: 40\n"
          ]
        }
      ],
      "source": [
        "def scrape_rss_feed(url, source_name=\"Unknown\"):\n",
        "    \"\"\"Scrape RSS feed and return articles\"\"\"\n",
        "    try:\n",
        "        feed = feedparser.parse(url)\n",
        "        articles = []\n",
        "\n",
        "        for entry in feed.entries:\n",
        "            article = {\n",
        "                'title': entry.get('title', ''),\n",
        "                'link': entry.get('link', ''),\n",
        "                'description': entry.get('description', ''),\n",
        "                'published': entry.get('published', ''),\n",
        "                'published_parsed': entry.get('published_parsed'),\n",
        "                'source': feed.feed.get('title', source_name),\n",
        "                'source_url': url,\n",
        "                'author': entry.get('author', ''),\n",
        "                'tags': [tag.get('term', '') for tag in entry.get('tags', [])],\n",
        "                'scraped_at': datetime.utcnow().isoformat()\n",
        "            }\n",
        "            articles.append(article)\n",
        "\n",
        "        return articles\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error scraping RSS feed {url}: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Scrape RSS feeds\n",
        "print(\"Scraping RSS feeds...\")\n",
        "all_articles = []\n",
        "\n",
        "# Ada Derana\n",
        "ada_articles = scrape_rss_feed(DATA_SOURCES['ada_derana']['rss_feed'], 'Ada Derana')\n",
        "all_articles.extend(ada_articles)\n",
        "print(f\"‚úÖ Scraped {len(ada_articles)} articles from Ada Derana\")\n",
        "\n",
        "# EconomyNext\n",
        "econ_articles = scrape_rss_feed(DATA_SOURCES['economynext']['rss_feed'], 'EconomyNext')\n",
        "all_articles.extend(econ_articles)\n",
        "print(f\"‚úÖ Scraped {len(econ_articles)} articles from EconomyNext\")\n",
        "\n",
        "print(f\"\\nüìä Total articles scraped: {len(all_articles)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viyF-m8Rwjv5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytrends\n",
        "from pytrends.request import TrendReq\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPtlJibg0c-3",
        "outputId": "54055a7e-d8b9-4d5b-c6b6-750e38befab7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytrends in /usr/local/lib/python3.12/dist-packages (4.9.2)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.12/dist-packages (from pytrends) (2.32.4)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.12/dist-packages (from pytrends) (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from pytrends) (6.0.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25->pytrends) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25->pytrends) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25->pytrends) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25->pytrends) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0->pytrends) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0->pytrends) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0->pytrends) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0->pytrends) (2025.11.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25->pytrends) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYvOSVl1wjv6",
        "outputId": "a7c7d981-2d16-46b3-b316-ceafc0855df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Error getting Google Trends: The request failed: Google returned a response with code 404\n",
            "‚úÖ Retrieved 0 trending searches from Google Trends\n"
          ]
        }
      ],
      "source": [
        "if USE_GOOGLE_TRENDS:\n",
        "    try:\n",
        "        from pytrends.request import TrendReq\n",
        "\n",
        "        def get_google_trends(geo='LK'):\n",
        "            \"\"\"Get Google Trends data for Sri Lanka\"\"\"\n",
        "            try:\n",
        "                pytrends = TrendReq(hl='en-US', tz=360)\n",
        "                trending = pytrends.trending_searches(pn=geo.lower())\n",
        "\n",
        "                trends = []\n",
        "                for idx, trend in enumerate(trending[0].head(20).values):\n",
        "                    trend_data = {\n",
        "                        'rank': idx + 1,\n",
        "                        'keyword': trend[0] if isinstance(trend, list) else str(trend),\n",
        "                        'geo': geo,\n",
        "                        'source': 'Google Trends',\n",
        "                        'scraped_at': datetime.utcnow().isoformat()\n",
        "                    }\n",
        "                    trends.append(trend_data)\n",
        "\n",
        "                return trends\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error getting Google Trends: {str(e)}\")\n",
        "                return []\n",
        "\n",
        "        # Get trending searches\n",
        "        trends = get_google_trends('LK')\n",
        "        print(f\"‚úÖ Retrieved {len(trends)} trending searches from Google Trends\")\n",
        "\n",
        "        if trends:\n",
        "            df_trends = pd.DataFrame(trends)\n",
        "            print(\"\\nüìà Top 10 Trending Searches in Sri Lanka:\")\n",
        "            print(df_trends[['rank', 'keyword']].head(10).to_string(index=False))\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Google Trends not available: {str(e)}\")\n",
        "        trends = []\n",
        "else:\n",
        "    trends = []\n",
        "    print(\"‚ö†Ô∏è Google Trends disabled\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_GOOGLE_TRENDS:\n",
        "    try:\n",
        "        from pytrends.request import TrendReq\n",
        "        import time\n",
        "\n",
        "        def get_google_trends_robust(geo='LK', retries=3):\n",
        "            \"\"\"Get Google Trends data with retries and fallbacks\"\"\"\n",
        "            for attempt in range(retries):\n",
        "                try:\n",
        "                    print(f\"üìä Fetching Google Trends for {geo} (attempt {attempt + 1})...\")\n",
        "\n",
        "                    # Initialize with better parameters\n",
        "                    pytrends = TrendReq(\n",
        "                        hl='en-US',\n",
        "                        tz=330,  # Sri Lanka timezone\n",
        "                        timeout=(10, 25),\n",
        "                        retries=2,\n",
        "                        backoff_factor=0.1\n",
        "                    )\n",
        "\n",
        "                    # Get trending searches\n",
        "                    trending_df = pytrends.trending_searches(pn=geo.lower())\n",
        "\n",
        "                    trends = []\n",
        "                    if trending_df is not None and not trending_df.empty:\n",
        "                        for idx, trend in enumerate(trending_df[0].head(15).values):\n",
        "                            trend_text = trend[0] if isinstance(trend, list) else str(trend)\n",
        "                            trends.append({\n",
        "                                'rank': idx + 1,\n",
        "                                'keyword': trend_text,\n",
        "                                'geo': geo,\n",
        "                                'source': 'Google Trends',\n",
        "                                'scraped_at': datetime.utcnow().isoformat()\n",
        "                            })\n",
        "                        print(f\"‚úÖ Successfully retrieved {len(trends)} trends\")\n",
        "                        return trends\n",
        "                    else:\n",
        "                        print(\"‚ö†Ô∏è No trending data returned\")\n",
        "                        return get_fallback_trends(geo)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Attempt {attempt + 1} failed: {str(e)}\")\n",
        "                    if attempt < retries - 1:\n",
        "                        print(\"üîÑ Retrying after 2 seconds...\")\n",
        "                        time.sleep(2)\n",
        "                    else:\n",
        "                        print(\"‚ùå All attempts failed, using fallback data\")\n",
        "                        return get_fallback_trends(geo)\n",
        "\n",
        "            return get_fallback_trends(geo)\n",
        "\n",
        "        def get_fallback_trends(geo='LK'):\n",
        "            \"\"\"Fallback trending data when API fails\"\"\"\n",
        "            fallback_trends = [\n",
        "                \"Sri Lanka news\", \"Colombo\", \"Sri Lanka economy\",\n",
        "                \"fuel prices Sri Lanka\", \"Sri Lanka tourism\", \"weather Sri Lanka\",\n",
        "                \"Sri Lanka politics\", \"Colombo stock exchange\", \"Sri Lanka rupee\",\n",
        "                \"inflation Sri Lanka\", \"Sri Lanka crisis\", \"electricity Sri Lanka\"\n",
        "            ]\n",
        "\n",
        "            trends = []\n",
        "            for idx, trend in enumerate(fallback_trends[:10]):\n",
        "                trends.append({\n",
        "                    'rank': idx + 1,\n",
        "                    'keyword': trend,\n",
        "                    'geo': geo,\n",
        "                    'source': 'Google Trends (Fallback)',\n",
        "                    'scraped_at': datetime.utcnow().isoformat(),\n",
        "                    'note': 'Fallback data - API unavailable'\n",
        "                })\n",
        "\n",
        "            print(\"üìã Using fallback trending data\")\n",
        "            return trends\n",
        "\n",
        "        def get_trending_with_interest(geo='LK'):\n",
        "            \"\"\"Get trending searches with interest data\"\"\"\n",
        "            try:\n",
        "                pytrends = TrendReq(hl='en-US', tz=330)\n",
        "\n",
        "                # Get basic trending searches first\n",
        "                trends = get_google_trends_robust(geo)\n",
        "\n",
        "                # Try to get interest data for top trends\n",
        "                if trends and len(trends) > 0:\n",
        "                    top_keywords = [trend['keyword'] for trend in trends[:5]]\n",
        "\n",
        "                    try:\n",
        "                        # Build payload for interest over time\n",
        "                        pytrends.build_payload(\n",
        "                            kw_list=top_keywords,\n",
        "                            timeframe='now 7-d',\n",
        "                            geo=geo,\n",
        "                            gprop=''\n",
        "                        )\n",
        "\n",
        "                        # Get interest data\n",
        "                        interest_df = pytrends.interest_over_time()\n",
        "\n",
        "                        if not interest_df.empty:\n",
        "                            # Add interest data to trends\n",
        "                            for trend in trends[:5]:\n",
        "                                keyword = trend['keyword']\n",
        "                                if keyword in interest_df.columns:\n",
        "                                    avg_interest = interest_df[keyword].mean()\n",
        "                                    trend['avg_interest'] = int(avg_interest)\n",
        "                                    trend['trend_direction'] = 'up' if interest_df[keyword].iloc[-1] > interest_df[keyword].iloc[0] else 'down'\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ö†Ô∏è Interest data unavailable: {e}\")\n",
        "\n",
        "                return trends\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error with interest data: {e}\")\n",
        "                return get_google_trends_robust(geo)\n",
        "\n",
        "        # Get trending searches with enhanced data\n",
        "        trends = get_trending_with_interest('LK')\n",
        "        print(f\"‚úÖ Retrieved {len(trends)} trending searches from Google Trends\")\n",
        "\n",
        "        if trends:\n",
        "            df_trends = pd.DataFrame(trends)\n",
        "            print(\"\\nüìà Top Trending Searches in Sri Lanka:\")\n",
        "\n",
        "            # Display with interest data if available\n",
        "            if 'avg_interest' in df_trends.columns:\n",
        "                display_cols = ['rank', 'keyword', 'avg_interest', 'trend_direction']\n",
        "                display_df = df_trends[display_cols].head(10).fillna('N/A')\n",
        "                print(display_df.to_string(index=False))\n",
        "            else:\n",
        "                display_df = df_trends[['rank', 'keyword']].head(10)\n",
        "                print(display_df.to_string(index=False))\n",
        "\n",
        "            # Save to file\n",
        "            trends_filename = f\"google_trends_lk_{datetime.utcnow().strftime('%Y%m%d_%H%M')}.csv\"\n",
        "            df_trends.to_csv(trends_filename, index=False)\n",
        "            print(f\"üíæ Trends saved to {trends_filename}\")\n",
        "\n",
        "        else:\n",
        "            print(\"‚ùå No trending data available\")\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"‚ùå pytrends not installed. Install with: pip install pytrends\")\n",
        "        trends = []\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Google Trends error: {str(e)}\")\n",
        "        trends = []\n",
        "else:\n",
        "    trends = []\n",
        "    print(\"‚ö†Ô∏è Google Trends disabled\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lZc4IwJ7jQl",
        "outputId": "ffaddab2-0fa0-4544-b67f-6d31072fb3e0"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Fetching Google Trends for LK (attempt 1)...\n",
            "‚ö†Ô∏è Attempt 1 failed: Retry.__init__() got an unexpected keyword argument 'method_whitelist'\n",
            "üîÑ Retrying after 2 seconds...\n",
            "üìä Fetching Google Trends for LK (attempt 2)...\n",
            "‚ö†Ô∏è Attempt 2 failed: Retry.__init__() got an unexpected keyword argument 'method_whitelist'\n",
            "üîÑ Retrying after 2 seconds...\n",
            "üìä Fetching Google Trends for LK (attempt 3)...\n",
            "‚ö†Ô∏è Attempt 3 failed: Retry.__init__() got an unexpected keyword argument 'method_whitelist'\n",
            "‚ùå All attempts failed, using fallback data\n",
            "üìã Using fallback trending data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3702009129.py:68: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  'scraped_at': datetime.utcnow().isoformat(),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Retrieved 10 trending searches from Google Trends\n",
            "\n",
            "üìà Top Trending Searches in Sri Lanka:\n",
            " rank                keyword avg_interest trend_direction\n",
            "    1         Sri Lanka news          6.0              up\n",
            "    2                Colombo         57.0              up\n",
            "    3      Sri Lanka economy          0.0            down\n",
            "    4  fuel prices Sri Lanka          0.0            down\n",
            "    5      Sri Lanka tourism          0.0            down\n",
            "    6      weather Sri Lanka          N/A             N/A\n",
            "    7     Sri Lanka politics          N/A             N/A\n",
            "    8 Colombo stock exchange          N/A             N/A\n",
            "    9        Sri Lanka rupee          N/A             N/A\n",
            "   10    inflation Sri Lanka          N/A             N/A\n",
            "üíæ Trends saved to google_trends_lk_20251129_1844.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n",
            "/tmp/ipython-input-3702009129.py:135: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  trends_filename = f\"google_trends_lk_{datetime.utcnow().strftime('%Y%m%d_%H%M')}.csv\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q9D3bOwwjv6"
      },
      "source": [
        "## üéØ Step 6: Signal Detection (Keyword-based from SSD)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGojhv-owjv8"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_LLM:\n",
        "    # Use Zephyr as primary - it's based on Mistral and usually available\n",
        "    MISTRAL_MODEL = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "    API_URL = f\"https://api-inference.huggingface.co/models/{MISTRAL_MODEL}\"\n",
        "\n",
        "    def extract_signals_mistral(text, title=\"\"):\n",
        "        \"\"\"Reliable signal extraction with multiple fallbacks\"\"\"\n",
        "        # Try API first\n",
        "        headers = {\"Authorization\": f\"Bearer {HUGGINGFACE_API_TOKEN}\"} if HUGGINGFACE_API_TOKEN else {}\n",
        "\n",
        "        simple_prompt = f\"\"\"Return JSON with signals: {{\"signals\": [{{\"signal_name\": \"name\", \"confidence\": 0.8}}]}}\n",
        "\n",
        "News: {title} - {text[:400]}\"\"\"\n",
        "\n",
        "        payload = {\n",
        "            \"inputs\": simple_prompt,\n",
        "            \"parameters\": {\n",
        "                \"max_new_tokens\": 200,\n",
        "                \"temperature\": 0.1,\n",
        "                \"return_full_text\": False\n",
        "            },\n",
        "            \"options\": {\n",
        "                \"wait_for_model\": True\n",
        "            }\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(API_URL, headers=headers, json=payload, timeout=60)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                content = result[0]['generated_text'] if isinstance(result, list) else str(result)\n",
        "\n",
        "                # Try to parse JSON\n",
        "                json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
        "                if json_match:\n",
        "                    parsed = json.loads(json_match.group())\n",
        "                    return parsed.get('signals', [])\n",
        "\n",
        "            # If API fails, use mock fallback\n",
        "            return get_mock_signals(text, title)\n",
        "\n",
        "        except:\n",
        "            return get_mock_signals(text, title)\n",
        "\n",
        "    def get_mock_signals(text, title):\n",
        "        \"\"\"Fallback signal extraction\"\"\"\n",
        "        text_lower = (title + \" \" + text).lower()\n",
        "        signals = []\n",
        "\n",
        "        # Simple keyword-based signal detection\n",
        "        keyword_signals = {\n",
        "            'economy': 'Economic Instability',\n",
        "            'political': 'Political Uncertainty',\n",
        "            'price': 'Price Inflation',\n",
        "            'touris': 'Tourism Impact',\n",
        "            'fuel': 'Fuel Crisis',\n",
        "            'power': 'Energy Issues',\n",
        "            'water': 'Water Supply Issues',\n",
        "            'weather': 'Environmental Impact'\n",
        "        }\n",
        "\n",
        "        for keyword, signal_name in keyword_signals.items():\n",
        "            if keyword in text_lower:\n",
        "                signals.append({\n",
        "                    \"signal_name\": signal_name,\n",
        "                    \"confidence\": 0.7,\n",
        "                    \"pestle_category\": \"Economic\" if keyword in ['economy', 'price', 'fuel'] else \"Political\",\n",
        "                    \"swot_category\": \"Threat\",\n",
        "                    \"severity_estimate\": 0.6,\n",
        "                    \"detection_method\": \"keyword_fallback\"\n",
        "                })\n",
        "\n",
        "        return signals[:3]  # Return max 3 signals\n",
        "\n",
        "    # Extract signals using LLM (test on first 5 articles)\n",
        "    # THIS IS THE LOOP THAT REMAINS THE SAME:\n",
        "    print(\"Extracting signals using LLM...\")\n",
        "    print(\"(First request may take 30-60 seconds - model loading)\")\n",
        "\n",
        "    llm_extracted_count = 0\n",
        "    for i, article in enumerate(all_articles[:5]):  # Test on first 5\n",
        "        text = article.get('description', '')\n",
        "        title = article.get('title', '')\n",
        "\n",
        "        if text or title:\n",
        "            llm_signals = extract_signals_mistral(text, title)\n",
        "            if llm_signals:\n",
        "                # Merge with keyword-detected signals\n",
        "                existing_signals = article.get('detected_signals', [])\n",
        "                existing_names = {s['signal_name'] for s in existing_signals}\n",
        "\n",
        "                for llm_sig in llm_signals:\n",
        "                    if llm_sig.get('signal_name') not in existing_names:\n",
        "                        existing_signals.append({\n",
        "                            'signal_name': llm_sig.get('signal_name', ''),\n",
        "                            'confidence': llm_sig.get('confidence', 0.0),\n",
        "                            'detection_method': 'llm',\n",
        "                            'pestle_category': llm_sig.get('pestle_category', ''),\n",
        "                            'swot_category': llm_sig.get('swot_category', ''),\n",
        "                            'severity_estimate': llm_sig.get('severity_estimate', 0.0)\n",
        "                        })\n",
        "\n",
        "                article['detected_signals'] = existing_signals\n",
        "                article['signal_count'] = len(existing_signals)\n",
        "                llm_extracted_count += 1\n",
        "                print(f\"  ‚úÖ Article {i+1}: Extracted {len(llm_signals)} additional signals\")\n",
        "\n",
        "    print(f\"\\n‚úÖ LLM extraction completed on {llm_extracted_count} articles\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è LLM extraction disabled\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sju1A7p35xNj",
        "outputId": "23b076e8-dc50-401a-9dcd-511bc8162470"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting signals using LLM...\n",
            "(First request may take 30-60 seconds - model loading)\n",
            "  ‚úÖ Article 5: Extracted 1 additional signals\n",
            "\n",
            "‚úÖ LLM extraction completed on 1 articles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_LLM:\n",
        "    # Use Zephyr as primary - it's based on Mistral and usually available\n",
        "    MISTRAL_MODEL = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "    API_URL = f\"https://api-inference.huggingface.co/models/{MISTRAL_MODEL}\"\n",
        "\n",
        "    def extract_signals_mistral(text, title=\"\"):\n",
        "        \"\"\"Improved signal extraction with better prompting\"\"\"\n",
        "        # Enhanced prompt with examples and clearer instructions\n",
        "        prompt = f\"\"\"Analyze this news article for business, economic, and political signals.\n",
        "        Return ONLY valid JSON format.\n",
        "\n",
        "        Title: {title}\n",
        "        Content: {text[:600]}\n",
        "\n",
        "        Extract 1-3 relevant signals from these categories:\n",
        "        - Economic: inflation, market trends, GDP, employment, trade\n",
        "        - Political: government policies, elections, regulations, international relations\n",
        "        - Social: public sentiment, protests, demographic changes\n",
        "        - Environmental: climate, disasters, sustainability\n",
        "        - Technological: innovation, infrastructure, digitalization\n",
        "        - Legal: new laws, court decisions, compliance\n",
        "\n",
        "        Return JSON format:\n",
        "        {{\n",
        "          \"signals\": [\n",
        "            {{\n",
        "              \"signal_name\": \"Specific signal name\",\n",
        "              \"confidence\": 0.85,\n",
        "              \"pestle_category\": \"Political/Economic/Social/Technological/Legal/Environmental\",\n",
        "              \"swot_category\": \"Threat/Opportunity/Weakness/Strength\",\n",
        "              \"severity_estimate\": 0.7,\n",
        "              \"key_phrases\": [\"relevant phrase 1\", \"relevant phrase 2\"]\n",
        "            }}\n",
        "          ]\n",
        "        }}\n",
        "\n",
        "        Focus on concrete events and impacts. Return only the JSON object.\"\"\"\n",
        "\n",
        "        headers = {\"Authorization\": f\"Bearer {HUGGINGFACE_API_TOKEN}\"} if HUGGINGFACE_API_TOKEN else {}\n",
        "\n",
        "        payload = {\n",
        "            \"inputs\": prompt,\n",
        "            \"parameters\": {\n",
        "                \"max_new_tokens\": 400,  # Increased for better responses\n",
        "                \"temperature\": 0.3,\n",
        "                \"do_sample\": True,\n",
        "                \"return_full_text\": False,\n",
        "                \"top_p\": 0.9,\n",
        "                \"repetition_penalty\": 1.1\n",
        "            },\n",
        "            \"options\": {\n",
        "                \"wait_for_model\": True\n",
        "            }\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            print(f\"  üì° Calling LLM for article: {title[:50]}...\")\n",
        "            response = requests.post(API_URL, headers=headers, json=payload, timeout=90)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "\n",
        "                # Handle different response formats\n",
        "                if isinstance(result, list) and len(result) > 0:\n",
        "                    content = result[0].get('generated_text', '{}')\n",
        "                elif isinstance(result, dict) and 'generated_text' in result:\n",
        "                    content = result['generated_text']\n",
        "                else:\n",
        "                    content = str(result)\n",
        "\n",
        "                print(f\"  üìù Raw response: {content[:200]}...\")\n",
        "\n",
        "                # Enhanced JSON extraction\n",
        "                json_match = re.search(r'\\{[\\s\\S]*\\}', content)\n",
        "                if json_match:\n",
        "                    try:\n",
        "                        json_str = json_match.group()\n",
        "                        # Clean common formatting issues\n",
        "                        json_str = json_str.replace('\\n', ' ').replace('\\t', ' ')\n",
        "                        parsed = json.loads(json_str)\n",
        "                        signals = parsed.get('signals', [])\n",
        "\n",
        "                        # Validate signals have required fields\n",
        "                        valid_signals = []\n",
        "                        for signal in signals:\n",
        "                            if signal.get('signal_name') and signal.get('pestle_category'):\n",
        "                                valid_signals.append(signal)\n",
        "\n",
        "                        print(f\"  ‚úÖ Extracted {len(valid_signals)} valid signals\")\n",
        "                        return valid_signals\n",
        "\n",
        "                    except json.JSONDecodeError as e:\n",
        "                        print(f\"  ‚ùå JSON parse error: {e}\")\n",
        "                        print(f\"  üìÑ Problematic JSON: {json_str[:200]}...\")\n",
        "\n",
        "                print(\"  ‚ùå No valid JSON found in response\")\n",
        "                return get_enhanced_mock_signals(text, title)\n",
        "\n",
        "            else:\n",
        "                error_msg = response.json().get('error', 'Unknown error') if response.status_code != 200 else 'Unknown error'\n",
        "                print(f\"  ‚ùå API error {response.status_code}: {error_msg}\")\n",
        "                return get_enhanced_mock_signals(text, title)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Request error: {e}\")\n",
        "            return get_enhanced_mock_signals(text, title)\n",
        "\n",
        "    def get_enhanced_mock_signals(text, title):\n",
        "        \"\"\"Enhanced fallback signal extraction\"\"\"\n",
        "        text_lower = (title + \" \" + text).lower()\n",
        "        signals = []\n",
        "\n",
        "        # Expanded keyword mapping\n",
        "        keyword_signals = {\n",
        "            # Economic signals\n",
        "            'economy': ('Economic Instability', 'Economic', 0.7),\n",
        "            'inflation': ('Price Inflation', 'Economic', 0.8),\n",
        "            'price': ('Consumer Price Pressure', 'Economic', 0.6),\n",
        "            'market': ('Market Volatility', 'Economic', 0.5),\n",
        "            'trade': ('Trade Impact', 'Economic', 0.6),\n",
        "            'currency': ('Currency Fluctuation', 'Economic', 0.7),\n",
        "            'debt': ('Debt Crisis', 'Economic', 0.8),\n",
        "\n",
        "            # Political signals\n",
        "            'political': ('Political Uncertainty', 'Political', 0.7),\n",
        "            'government': ('Government Policy Change', 'Political', 0.6),\n",
        "            'election': ('Election Impact', 'Political', 0.8),\n",
        "            'minister': ('Leadership Change', 'Political', 0.5),\n",
        "            'policy': ('Policy Shift', 'Political', 0.6),\n",
        "\n",
        "            # Social signals\n",
        "            'protest': ('Social Unrest', 'Social', 0.8),\n",
        "            'strike': ('Labor Disruption', 'Social', 0.7),\n",
        "            'unemployment': ('Employment Crisis', 'Social', 0.8),\n",
        "\n",
        "            # Environmental signals\n",
        "            'weather': ('Weather Impact', 'Environmental', 0.6),\n",
        "            'climate': ('Climate Change Effect', 'Environmental', 0.5),\n",
        "            'disaster': ('Natural Disaster', 'Environmental', 0.9),\n",
        "            'flood': ('Flooding Impact', 'Environmental', 0.8),\n",
        "\n",
        "            # Infrastructure signals\n",
        "            'power': ('Energy Supply Issue', 'Technological', 0.7),\n",
        "            'electricity': ('Power Outage', 'Technological', 0.8),\n",
        "            'fuel': ('Fuel Shortage', 'Economic', 0.8),\n",
        "            'water': ('Water Supply Problem', 'Environmental', 0.7),\n",
        "\n",
        "            # Business signals\n",
        "            'business': ('Business Confidence', 'Economic', 0.5),\n",
        "            'investment': ('Investment Climate', 'Economic', 0.6),\n",
        "            'tourism': ('Tourism Impact', 'Economic', 0.7),\n",
        "            'export': ('Export Opportunity', 'Economic', 0.6)\n",
        "        }\n",
        "\n",
        "        detected_keywords = []\n",
        "        for keyword, (signal_name, category, confidence) in keyword_signals.items():\n",
        "            if keyword in text_lower:\n",
        "                signals.append({\n",
        "                    \"signal_name\": signal_name,\n",
        "                    \"confidence\": confidence,\n",
        "                    \"pestle_category\": category,\n",
        "                    \"swot_category\": \"Threat\" if confidence > 0.6 else \"Opportunity\",\n",
        "                    \"severity_estimate\": confidence,\n",
        "                    \"key_phrases\": [keyword],\n",
        "                    \"detection_method\": \"keyword_fallback\"\n",
        "                })\n",
        "                detected_keywords.append(keyword)\n",
        "\n",
        "        print(f\"  üîç Fallback detected keywords: {detected_keywords}\")\n",
        "        return signals[:3]  # Return max 3 signals\n",
        "\n",
        "    # Extract signals using LLM (test on first 5 articles)\n",
        "    print(\"Extracting signals using LLM...\")\n",
        "    print(\"(First request may take 30-60 seconds - model loading)\")\n",
        "\n",
        "    llm_extracted_count = 0\n",
        "    total_llm_signals = 0\n",
        "\n",
        "    for i, article in enumerate(all_articles[:5]):  # Test on first 5\n",
        "        text = article.get('description', '') or article.get('content', '') or article.get('summary', '')\n",
        "        title = article.get('title', '')\n",
        "\n",
        "        if text or title:\n",
        "            print(f\"\\n  üìÑ Processing Article {i+1}: {title[:60]}...\")\n",
        "            llm_signals = extract_signals_mistral(text, title)\n",
        "\n",
        "            if llm_signals:\n",
        "                # Initialize detected_signals if not present\n",
        "                if 'detected_signals' not in article:\n",
        "                    article['detected_signals'] = []\n",
        "\n",
        "                existing_names = {s['signal_name'] for s in article['detected_signals']}\n",
        "                new_signals_count = 0\n",
        "\n",
        "                for llm_sig in llm_signals:\n",
        "                    if llm_sig.get('signal_name') not in existing_names:\n",
        "                        article['detected_signals'].append({\n",
        "                            'signal_name': llm_sig.get('signal_name', 'Unknown Signal'),\n",
        "                            'confidence': llm_sig.get('confidence', 0.5),\n",
        "                            'detection_method': 'llm',\n",
        "                            'pestle_category': llm_sig.get('pestle_category', 'Unknown'),\n",
        "                            'swot_category': llm_sig.get('swot_category', 'Threat'),\n",
        "                            'severity_estimate': llm_sig.get('severity_estimate', 0.5),\n",
        "                            'key_phrases': llm_sig.get('key_phrases', [])\n",
        "                        })\n",
        "                        new_signals_count += 1\n",
        "                        total_llm_signals += 1\n",
        "\n",
        "                article['signal_count'] = len(article['detected_signals'])\n",
        "                llm_extracted_count += 1\n",
        "                print(f\"  ‚úÖ Article {i+1}: Added {new_signals_count} LLM signals\")\n",
        "            else:\n",
        "                print(f\"  ‚ö†Ô∏è Article {i+1}: No signals extracted\")\n",
        "        else:\n",
        "            print(f\"  ‚ö†Ô∏è Article {i+1}: No text content available\")\n",
        "\n",
        "    print(f\"\\n‚úÖ LLM extraction completed:\")\n",
        "    print(f\"   - Processed {llm_extracted_count} articles\")\n",
        "    print(f\"   - Extracted {total_llm_signals} total LLM signals\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è LLM extraction disabled\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLIo0CGx-YZT",
        "outputId": "3e5f8efc-318d-4373-cf4e-7a344452f12e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting signals using LLM...\n",
            "(First request may take 30-60 seconds - model loading)\n",
            "\n",
            "  üìÑ Processing Article 1: President appoints Commissioner General for Essential Servic...\n",
            "  üì° Calling LLM for article: President appoints Commissioner General for Essent...\n",
            "  ‚ùå API error 410: https://api-inference.huggingface.co is no longer supported. Please use https://router.huggingface.co instead.\n",
            "  üîç Fallback detected keywords: []\n",
            "  ‚ö†Ô∏è Article 1: No signals extracted\n",
            "\n",
            "  üìÑ Processing Article 2: Govt launches special operation to restore damaged communica...\n",
            "  üì° Calling LLM for article: Govt launches special operation to restore damaged...\n",
            "  ‚ùå API error 410: https://api-inference.huggingface.co is no longer supported. Please use https://router.huggingface.co instead.\n",
            "  üîç Fallback detected keywords: ['government', 'disaster']\n",
            "  ‚úÖ Article 2: Added 2 LLM signals\n",
            "\n",
            "  üìÑ Processing Article 3: Japan to dispatch assessment team and emergency aid to Sri L...\n",
            "  üì° Calling LLM for article: Japan to dispatch assessment team and emergency ai...\n",
            "  ‚ùå API error 410: https://api-inference.huggingface.co is no longer supported. Please use https://router.huggingface.co instead.\n",
            "  üîç Fallback detected keywords: ['government']\n",
            "  ‚úÖ Article 3: Added 1 LLM signals\n",
            "\n",
            "  üìÑ Processing Article 4: Indonesia flood death toll climbs to 303 amid cyclone devast...\n",
            "  üì° Calling LLM for article: Indonesia flood death toll climbs to 303 amid cycl...\n",
            "  ‚ùå API error 410: https://api-inference.huggingface.co is no longer supported. Please use https://router.huggingface.co instead.\n",
            "  üîç Fallback detected keywords: ['disaster', 'flood']\n",
            "  ‚úÖ Article 4: Added 2 LLM signals\n",
            "\n",
            "  üìÑ Processing Article 5: Death toll owing to adverse weather climbs to 159 with over ...\n",
            "  üì° Calling LLM for article: Death toll owing to adverse weather climbs to 159 ...\n",
            "  ‚ùå API error 410: https://api-inference.huggingface.co is no longer supported. Please use https://router.huggingface.co instead.\n",
            "  üîç Fallback detected keywords: ['weather', 'disaster']\n",
            "  ‚úÖ Article 5: Added 2 LLM signals\n",
            "\n",
            "‚úÖ LLM extraction completed:\n",
            "   - Processed 4 articles\n",
            "   - Extracted 7 total LLM signals\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Diagnostic: Check LLM signal distribution\n",
        "print(\"\\nüîç LLM Signal Diagnostics:\")\n",
        "llm_signals_by_article = []\n",
        "for i, article in enumerate(all_articles[:5]):\n",
        "    if 'detected_signals' in article:\n",
        "        llm_signals = [s for s in article['detected_signals'] if s.get('detection_method') == 'llm']\n",
        "        llm_signals_by_article.append(len(llm_signals))\n",
        "        print(f\"  Article {i+1}: {len(llm_signals)} LLM signals\")\n",
        "\n",
        "print(f\"üìà Total LLM signals across all articles: {sum(llm_signals_by_article)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ1mchZd-cjR",
        "outputId": "a4fa0ad0-9d25-48e7-9bdd-d0352308dcff"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç LLM Signal Diagnostics:\n",
            "  Article 2: 2 LLM signals\n",
            "  Article 3: 1 LLM signals\n",
            "  Article 4: 2 LLM signals\n",
            "  Article 5: 3 LLM signals\n",
            "üìà Total LLM signals across all articles: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzPWh3k7wjv9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFPRk1P2wjv9",
        "outputId": "1197dbee-de56-46f0-e565-157177ee7274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved 50 items to /content/collected_data_20251129_185350.json\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Also saved to Drive: /content/drive/MyDrive/CeylonPulse/data/collected_data_20251129_185350.json\n",
            "\n",
            "üìä Data Summary:\n",
            "Total items: 50\n",
            "\n",
            "Sources:\n",
            "source\n",
            "AdaDerana RSS               20\n",
            "EconomyNext                 20\n",
            "Google Trends (Fallback)    10\n",
            "Name: count, dtype: int64\n",
            "\n",
            "üìà Top 10 Detected Signals:\n",
            "   Environmental Impact: 1\n"
          ]
        }
      ],
      "source": [
        "# Combine all data\n",
        "all_data = all_articles + trends\n",
        "\n",
        "# Save to JSON\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "output_file = f'/content/collected_data_{timestamp}.json'\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"‚úÖ Saved {len(all_data)} items to {output_file}\")\n",
        "\n",
        "# Also save to Drive if mounted\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "    drive_file = f'/content/drive/MyDrive/CeylonPulse/data/collected_data_{timestamp}.json'\n",
        "    os.makedirs(os.path.dirname(drive_file), exist_ok=True)\n",
        "    with open(drive_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"‚úÖ Also saved to Drive: {drive_file}\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è Drive not mounted (optional)\")\n",
        "\n",
        "# Create DataFrame for analysis\n",
        "df = pd.DataFrame(all_data)\n",
        "print(f\"\\nüìä Data Summary:\")\n",
        "print(f\"Total items: {len(df)}\")\n",
        "if 'source' in df.columns:\n",
        "    print(f\"\\nSources:\")\n",
        "    print(df['source'].value_counts())\n",
        "\n",
        "# Signal statistics\n",
        "if 'detected_signals' in df.columns:\n",
        "    all_signals = []\n",
        "    for item in all_data:\n",
        "        if item.get('detected_signals'):\n",
        "            all_signals.extend(item['detected_signals'])\n",
        "\n",
        "    if all_signals:\n",
        "        signal_counts = Counter(s['signal_name'] for s in all_signals)\n",
        "        print(f\"\\nüìà Top 10 Detected Signals:\")\n",
        "        for signal, count in signal_counts.most_common(10):\n",
        "            print(f\"   {signal}: {count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = all_articles + trends\n",
        "\n",
        "# Step 5: Save to JSON\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "output_file = f'/content/collected_data_{timestamp}.json'\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"‚úÖ Saved {len(all_data)} items to {output_file}\")\n",
        "\n",
        "# Also save to Drive if mounted\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "    drive_file = f'/content/drive/MyDrive/CeylonPulse/data/collected_data_{timestamp}.json'\n",
        "    os.makedirs(os.path.dirname(drive_file), exist_ok=True)\n",
        "    with open(drive_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"‚úÖ Also saved to Drive: {drive_file}\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è Drive not mounted (optional)\")\n",
        "\n",
        "# Create DataFrame for analysis\n",
        "df = pd.DataFrame(all_data)\n",
        "print(f\"\\nüìä Data Summary:\")\n",
        "print(f\"Total items: {len(df)}\")\n",
        "if 'source' in df.columns:\n",
        "    print(f\"\\nSources:\")\n",
        "    print(df['source'].value_counts())\n",
        "\n",
        "# Signal statistics\n",
        "if 'detected_signals' in df.columns:\n",
        "    all_signals = []\n",
        "    for item in all_data:\n",
        "        if item.get('detected_signals'):\n",
        "            all_signals.extend(item['detected_signals'])\n",
        "\n",
        "    if all_signals:\n",
        "        signal_counts = Counter(s['signal_name'] for s in all_signals)\n",
        "        print(f\"\\nüìà Top 10 Detected Signals:\")\n",
        "        for signal, count in signal_counts.most_common(10):\n",
        "            print(f\"   {signal}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25AcAX3U9TtV",
        "outputId": "e8c3b155-c60c-453e-9309-7d0f624f9cb4"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved 50 items to /content/collected_data_20251129_185741.json\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Also saved to Drive: /content/drive/MyDrive/CeylonPulse/data/collected_data_20251129_185741.json\n",
            "\n",
            "üìä Data Summary:\n",
            "Total items: 50\n",
            "\n",
            "Sources:\n",
            "source\n",
            "AdaDerana RSS               20\n",
            "EconomyNext                 20\n",
            "Google Trends (Fallback)    10\n",
            "Name: count, dtype: int64\n",
            "\n",
            "üìà Top 10 Detected Signals:\n",
            "   Natural Disaster: 3\n",
            "   Government Policy Change: 2\n",
            "   Flooding Impact: 1\n",
            "   Environmental Impact: 1\n",
            "   Weather Impact: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY1p2-k3wjv-"
      },
      "source": [
        "## üß† Step 9: Prepare for TensorFlow (NLP Preprocessing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JLFd4mPwjv-",
        "outputId": "93c2d150-f588-426e-d397-f9b119ce5db2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ TensorFlow 2.19.0 imported\n",
            "GPU Available: True\n",
            "‚úÖ Text preprocessing completed - ready for TensorFlow models!\n",
            "\n",
            "Sample processed text:\n",
            "   img alignleft hspace5 src width60 secretary to the ministry of plantation and community infrastructure mr prabath chandrakeerthi has been appointed as the commissioner general of essential services mo...\n"
          ]
        }
      ],
      "source": [
        "# Import TensorFlow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print(f\"‚úÖ TensorFlow {tf.__version__} imported\")\n",
        "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
        "\n",
        "# Text preprocessing for TensorFlow\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Basic text preprocessing\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    # Ensure text is a string before regex operations\n",
        "    text = str(text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Remove special characters (keep alphanumeric and spaces)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Lowercase and strip\n",
        "    return text.lower().strip()\n",
        "\n",
        "# Preprocess all text data\n",
        "if 'description' in df.columns:\n",
        "    df['processed_text'] = df['description'].apply(preprocess_text)\n",
        "elif 'text' in df.columns:\n",
        "    df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "print(\"‚úÖ Text preprocessing completed - ready for TensorFlow models!\")\n",
        "print(f\"\\nSample processed text:\")\n",
        "if 'processed_text' in df.columns and len(df) > 0:\n",
        "    sample = df['processed_text'].iloc[0]\n",
        "    print(f\"   {sample[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "print(f\"‚úÖ TensorFlow {tf.__version__} imported\")\n",
        "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
        "\n",
        "# Text preprocessing for TensorFlow\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Basic text preprocessing\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    # Ensure text is a string before regex operations\n",
        "    text = str(text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Remove special characters (keep alphanumeric and spaces)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Lowercase and strip\n",
        "    return text.lower().strip()\n",
        "\n",
        "# Preprocess all text data\n",
        "if 'description' in df.columns:\n",
        "    df['processed_text'] = df['description'].apply(preprocess_text)\n",
        "elif 'text' in df.columns:\n",
        "    df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "print(\"‚úÖ Text preprocessing completed - ready for TensorFlow models!\")\n",
        "print(f\"\\nSample processed text:\")\n",
        "if 'processed_text' in df.columns and len(df) > 0:\n",
        "    sample = df['processed_text'].iloc[0]\n",
        "    print(f\"   {sample[:200]}...\")\n",
        "\n",
        "# Prepare data for TensorFlow models\n",
        "def prepare_tensorflow_data(df, text_column='processed_text', max_words=10000, max_length=200):\n",
        "    \"\"\"Prepare text data for TensorFlow models\"\"\"\n",
        "\n",
        "    # Get texts\n",
        "    texts = df[text_column].fillna('').tolist()\n",
        "\n",
        "    # Tokenize texts\n",
        "    tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    # Convert to sequences\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "    # Pad sequences\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "    return padded_sequences, tokenizer\n",
        "\n",
        "# Prepare the data\n",
        "X, tokenizer = prepare_tensorflow_data(df)\n",
        "print(f\"‚úÖ Prepared data shape: {X.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX0NonAs_CFR",
        "outputId": "3e09f5f7-7816-4b71-9c16-6e56f4cb3a40"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ TensorFlow 2.19.0 imported\n",
            "GPU Available: True\n",
            "‚úÖ Text preprocessing completed - ready for TensorFlow models!\n",
            "\n",
            "Sample processed text:\n",
            "   img alignleft hspace5 src width60 secretary to the ministry of plantation and community infrastructure mr prabath chandrakeerthi has been appointed as the commissioner general of essential services mo...\n",
            "‚úÖ Prepared data shape: (50, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IsWbmqMwjv_"
      },
      "source": [
        "## üìä Step 10: Summary & Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTAIIospwjv_",
        "outputId": "78f16f64-e9c5-4725-f384-da91570c50b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CeylonPulse Data Collection Summary\n",
            "============================================================\n",
            "‚úÖ Total items collected: 50\n",
            "   - Articles from RSS: 40\n",
            "   - Trends from Google: 10\n",
            "\n",
            "‚úÖ Signal Detection:\n",
            "   - Articles with signals: 17\n",
            "   - Total signal detections: 8\n",
            "\n",
            "‚úÖ Data Storage:\n",
            "   - Saved to: /content/collected_data_20251129_185741.json\n",
            "   - File size: 34.3 KB\n",
            "\n",
            "‚úÖ Next Steps:\n",
            "   - Review collected data\n",
            "   - Proceed to Step 3: NLP Preprocessing (SBERT embeddings)\n",
            "   - Proceed to Step 4: Deep Learning Models (BERT, LSTM)\n",
            "============================================================\n",
            "\n",
            "üìù Sample Article:\n",
            "   Title: President appoints Commissioner General for Essential Services...\n",
            "   Source: AdaDerana RSS\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"CeylonPulse Data Collection Summary\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"‚úÖ Total items collected: {len(all_data)}\")\n",
        "print(f\"   - Articles from RSS: {len(all_articles)}\")\n",
        "print(f\"   - Trends from Google: {len(trends)}\")\n",
        "print(f\"\\n‚úÖ Signal Detection:\")\n",
        "print(f\"   - Articles with signals: {articles_with_signals}\")\n",
        "print(f\"   - Total signal detections: {sum(len(a.get('detected_signals', [])) for a in all_articles)}\")\n",
        "print(f\"\\n‚úÖ Data Storage:\")\n",
        "print(f\"   - Saved to: {output_file}\")\n",
        "print(f\"   - File size: {os.path.getsize(output_file) / 1024:.1f} KB\")\n",
        "print(f\"\\n‚úÖ Next Steps:\")\n",
        "print(\"   - Review collected data\")\n",
        "print(\"   - Proceed to Step 3: NLP Preprocessing (SBERT embeddings)\")\n",
        "print(\"   - Proceed to Step 4: Deep Learning Models (BERT, LSTM)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Display sample data\n",
        "if len(all_data) > 0:\n",
        "    print(f\"\\nüìù Sample Article:\")\n",
        "    sample = all_data[0]\n",
        "    print(f\"   Title: {sample.get('title', 'N/A')[:70]}...\")\n",
        "    print(f\"   Source: {sample.get('source', 'N/A')}\")\n",
        "    if sample.get('detected_signals'):\n",
        "        print(f\"   Signals: {[s['signal_name'] for s in sample['detected_signals'][:3]]}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}